{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Coding Part - Descriptive Statistics & Machine Learning"
      ],
      "metadata": {
        "id": "kivqDn6jIRkx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVANtQMRJ_3r"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"spark-3.2.0-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "urudg0X-KBAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "Ga1j9vq-KB_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "sc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "nfVXxrkQKDIF",
        "outputId": "cfca8d31-5437-49b2-ecf3-a5fefec2db22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SparkContext master=local[*] appName=pyspark-shell>"
            ],
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://e5d6e2cafcfa:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import RegressionEvaluator"
      ],
      "metadata": {
        "id": "B8W8d28kn7cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ytxqwvf8KEWa",
        "outputId": "e5ccf934-bf70-4031-ea2a-8e47516280eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file_path = \"/content/gdrive/MyDrive/cc_project/Boston.csv\""
      ],
      "metadata": {
        "id": "6k_b4_OxKF5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "vIboYQJ0KHCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reading the csv\n",
        "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(csv_file_path)"
      ],
      "metadata": {
        "id": "1Yiur5-NKIdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGCQYD4QpZrP",
        "outputId": "2f500866-3f48-495c-d358-085e7950dacf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('crim', 'double'),\n",
              " ('zn', 'double'),\n",
              " ('indus', 'double'),\n",
              " ('chas', 'int'),\n",
              " ('nox', 'double'),\n",
              " ('rm', 'double'),\n",
              " ('age', 'double'),\n",
              " ('dis', 'double'),\n",
              " ('rad', 'int'),\n",
              " ('tax', 'int'),\n",
              " ('ptratio', 'double'),\n",
              " ('b', 'double'),\n",
              " ('lstat', 'double'),\n",
              " ('medv', 'double')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# target = input(\"Please enter the value for Target variable name: \")"
      ],
      "metadata": {
        "id": "ehpE27uHKMPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#clean column names -> column names with no special characters\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import regexp_replace\n",
        "\n",
        "def clean_column_names(df, replacement_char='_'):\n",
        "    # spark = SparkSession.builder.appName(\"Column Name Cleaner\").getOrCreate()\n",
        "    cleaned_column_names = [regexp_replace(col, r'[^a-zA-Z0-9-_]+', replacement_char).alias(col) for col in df.columns]\n",
        "    df_cleaned = df.select(*cleaned_column_names)\n",
        "    return df_cleaned\n",
        "\n",
        "#df = clean_column_names(df)"
      ],
      "metadata": {
        "id": "FdMCxjCnKO_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#read and get first five rows\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "def read_and_get_first_5_rows(df):\n",
        "    # spark = SparkSession.builder.appName(\"Get First 5 Rows\").getOrCreate()\n",
        "    first_5_rows = df.limit(5)\n",
        "    return first_5_rows\n",
        "\n",
        "first_5_rows = read_and_get_first_5_rows(df)"
      ],
      "metadata": {
        "id": "7WMHNLYdKSHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the length of the dataframe\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "def get_dataframe_length(df):\n",
        "    # spark = SparkSession.builder.appName(\"DataFrame Length\").getOrCreate()\n",
        "    # Use the count action to get the length of the DataFrame\n",
        "    length = df.count()\n",
        "\n",
        "    return length\n",
        "\n",
        "length = get_dataframe_length(df)"
      ],
      "metadata": {
        "id": "e7W9l99ZKT7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the info of the dataframe\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "def info(df):\n",
        "    #spark = SparkSession.builder.appName(\"DataFrame Info\").getOrCreate()\n",
        "    df.printSchema()\n",
        "\n",
        "# info(df)"
      ],
      "metadata": {
        "id": "JxrsGB7tKXUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the dataframe description\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "def get_dataframe_description(df):\n",
        "    #spark = SparkSession.builder.appName(\"DataFrame Description\").getOrCreate()\n",
        "    description = df.describe()\n",
        "    return description\n",
        "\n",
        "#description = get_dataframe_description(df)"
      ],
      "metadata": {
        "id": "Jxj_LOq5KY9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#removing duplicate rows\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "def remove_duplicate_rows(df):\n",
        "    #spark = SparkSession.builder.appName(\"Remove Duplicate Rows\").getOrCreate()\n",
        "    df = df.dropDuplicates()\n",
        "    return df\n",
        "\n",
        "df = remove_duplicate_rows(df)"
      ],
      "metadata": {
        "id": "02loJhtMKbN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the categorical columns\n",
        "categorical_cols = [col_name for col_name, dtype in df.dtypes if dtype == \"string\"]\n",
        "categorical_description_output = df.select(categorical_cols).describe()\n",
        "# categorical_description_output.show()"
      ],
      "metadata": {
        "id": "pSU0Nn9OKcu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql.functions import col"
      ],
      "metadata": {
        "id": "ghf2-_yMKeek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#missing values count of each column\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, sum as spark_sum\n",
        "def get_missing_values_count(df):\n",
        "    missing_values_count = df.select(*[spark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
        "    return missing_values_count"
      ],
      "metadata": {
        "id": "O9U5BUWiKjvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_count = get_missing_values_count(df)"
      ],
      "metadata": {
        "id": "5iDDZfbjKmXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_count.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8etXsXoZGLXm",
        "outputId": "2f92b91d-df47-4d8e-dacf-5be565ccc893"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+-----+----+---+---+---+---+---+---+-------+---+-----+----+\n",
            "|crim| zn|indus|chas|nox| rm|age|dis|rad|tax|ptratio|  b|lstat|medv|\n",
            "+----+---+-----+----+---+---+---+---+---+---+-------+---+-----+----+\n",
            "|   0|  0|    0|   0|  0|  0|  0|  0|  0|  0|      0|  0|    0|   0|\n",
            "+----+---+-----+----+---+---+---+---+---+---+-------+---+-----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#handling missing values imputing with threshold & deleting missing values based on threshold\n",
        "def clean_dataframe(df, null_threshold=0.3, fill_missing=True):\n",
        "\n",
        "    total_rows = df.count()\n",
        "    threshold = null_threshold * total_rows\n",
        "\n",
        "    # Find columns to delete\n",
        "    columns_to_delete = [col_name for col_name in df.columns if df.filter(col(col_name).isNull()).count() > threshold]\n",
        "\n",
        "    # Drop columns with too many NULL values\n",
        "    cleaned_df = df.drop(*columns_to_delete)\n",
        "\n",
        "    if fill_missing:\n",
        "        for col_name, data_type in cleaned_df.dtypes:\n",
        "            null_count = cleaned_df.filter(col(col_name).isNull()).count()\n",
        "            if null_count > 0:\n",
        "                if data_type in ['string', 'object']:\n",
        "                    mode_value = cleaned_df.groupBy().agg({col_name: 'max'}).collect()[0][0]\n",
        "                    cleaned_df = cleaned_df.fillna({col_name: mode_value})\n",
        "                elif data_type in ['int', 'int32', 'int64', 'float', 'float32', 'float64']:\n",
        "                    median_value = cleaned_df.approxQuantile(col_name, [0.5], 0.25)[0]\n",
        "                    cleaned_df = cleaned_df.fillna({col_name: median_value})\n",
        "\n",
        "    return cleaned_df"
      ],
      "metadata": {
        "id": "EIw3rdN0Kgom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df = clean_dataframe(df)"
      ],
      "metadata": {
        "id": "FrT4ri2cKiQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#returns categorical and quantitative columns\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "def get_categorical_and_quantitative_columns(df):\n",
        "    categorical_columns = []\n",
        "    quantitative_columns = []\n",
        "\n",
        "    for column in df.columns:\n",
        "        if df.schema[column].dataType == StringType():\n",
        "            categorical_columns.append(column)\n",
        "        else:\n",
        "            quantitative_columns.append(column)\n",
        "\n",
        "    return categorical_columns, quantitative_columns"
      ],
      "metadata": {
        "id": "tNkjCOfmKn4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encoding the categorical variables using label encoding\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "def encode_categorical_variables(df):\n",
        "    categorical_columns, _ = get_categorical_and_quantitative_columns(df)\n",
        "\n",
        "    for column in categorical_columns:\n",
        "        indexer = StringIndexer(inputCol=column, outputCol=column+\"_enc\", handleInvalid=\"skip\")\n",
        "        df = indexer.fit(df).transform(df)\n",
        "        df = df.drop(column)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "n52CqVbJKpbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_df = encode_categorical_variables(cleaned_df)"
      ],
      "metadata": {
        "id": "w9Su9V9KKr1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QA6q0WRdhuq",
        "outputId": "2a685005-8b53-4eea-f97b-3e3590d1d898"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+-----+----+------+-----+-----+------+---+---+-------+------+-----+----+\n",
            "|   crim|  zn|indus|chas|   nox|   rm|  age|   dis|rad|tax|ptratio|     b|lstat|medv|\n",
            "+-------+----+-----+----+------+-----+-----+------+---+---+-------+------+-----+----+\n",
            "|0.62976| 0.0| 8.14|   0| 0.538|5.949| 61.8|4.7075|  4|307|   21.0| 396.9| 8.26|20.4|\n",
            "|0.22188|20.0| 6.96|   1| 0.464|7.691| 51.8|4.3665|  3|223|   18.6|390.77| 6.58|35.2|\n",
            "|0.06899| 0.0|25.65|   0| 0.581| 5.87| 69.7|2.2577|  2|188|   19.1|389.15|14.37|22.0|\n",
            "|0.97617| 0.0|21.89|   0| 0.624|5.757| 98.4| 2.346|  4|437|   21.2|262.76|17.31|15.6|\n",
            "|0.22876| 0.0| 8.56|   0|  0.52|6.405| 85.4|2.7147|  5|384|   20.9|  70.8|10.63|18.6|\n",
            "|  2.924| 0.0|19.58|   0| 0.605|6.101| 93.0|2.2834|  5|403|   14.7|240.16| 9.81|25.0|\n",
            "|0.03578|20.0| 3.33|   0|0.4429| 7.82| 64.5|4.6947|  5|216|   14.9|387.31| 3.76|45.4|\n",
            "|0.05372| 0.0|13.92|   0| 0.437|6.549| 51.0|5.9604|  4|289|   16.0|392.85| 7.39|27.1|\n",
            "|0.03961| 0.0| 5.19|   0| 0.515|6.037| 34.5|5.9853|  5|224|   20.2| 396.9| 8.01|21.1|\n",
            "|14.0507| 0.0| 18.1|   0| 0.597|6.657|100.0|1.5275| 24|666|   20.2| 35.05|21.22|17.2|\n",
            "|5.66998| 0.0| 18.1|   1| 0.631|6.683| 96.8|1.3567| 24|666|   20.2|375.33| 3.73|50.0|\n",
            "|1.15172| 0.0| 8.14|   0| 0.538|5.701| 95.0|3.7872|  4|307|   21.0|358.77|18.35|13.1|\n",
            "|0.07022| 0.0| 4.05|   0|  0.51| 6.02| 47.2|3.5549|  5|296|   16.6|393.23|10.11|23.2|\n",
            "|0.03445|82.5| 2.03|   0| 0.415|6.162| 38.4|  6.27|  2|348|   14.7|393.77| 7.43|24.1|\n",
            "|0.03427| 0.0| 5.19|   0| 0.515|5.869| 46.3|5.2311|  5|224|   20.2| 396.9|  9.8|19.5|\n",
            "|4.66883| 0.0| 18.1|   0| 0.713|5.976| 87.9|2.5806| 24|666|   20.2| 10.48|19.01|12.7|\n",
            "|2.24236| 0.0|19.58|   0| 0.605|5.854| 91.8| 2.422|  5|403|   14.7|395.11|11.64|22.7|\n",
            "|7.67202| 0.0| 18.1|   0| 0.693|5.747| 98.9|1.6334| 24|666|   20.2| 393.1|19.92| 8.5|\n",
            "|28.6558| 0.0| 18.1|   0| 0.597|5.155|100.0|1.5894| 24|666|   20.2|210.97|20.08|16.3|\n",
            "|0.08014| 0.0| 5.96|   0| 0.499| 5.85| 41.5|3.9342|  5|279|   19.2| 396.9| 8.77|21.0|\n",
            "+-------+----+-----+----+------+-----+-----+------+---+---+-------+------+-----+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_df.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mr38Q-XzkewZ",
        "outputId": "f1dd59d9-a18b-40b0-a50c-94c78f58d42f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('crim', 'double'),\n",
              " ('zn', 'double'),\n",
              " ('indus', 'double'),\n",
              " ('chas', 'int'),\n",
              " ('nox', 'double'),\n",
              " ('rm', 'double'),\n",
              " ('age', 'double'),\n",
              " ('dis', 'double'),\n",
              " ('rad', 'int'),\n",
              " ('tax', 'int'),\n",
              " ('ptratio', 'double'),\n",
              " ('b', 'double'),\n",
              " ('lstat', 'double'),\n",
              " ('medv', 'double')]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#code for regression model Machine Learning\n",
        "def calculate_rmse(encoded_df, target):\n",
        "    feature_columns = [col for col in encoded_df.columns if col != target]\n",
        "    encoded_df = encoded_df.dropna(subset=feature_columns)\n",
        "    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "    encoded_df = assembler.transform(encoded_df)\n",
        "    train_data, test_data = encoded_df.randomSplit([0.8, 0.2], seed=123)\n",
        "    lr = LinearRegression(featuresCol=\"features\", labelCol=target)\n",
        "    lr_model = lr.fit(train_data)\n",
        "    predictions = lr_model.transform(test_data)\n",
        "    evaluator = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "    rmse = evaluator.evaluate(predictions)\n",
        "    return rmse"
      ],
      "metadata": {
        "id": "zBffjVo4nnGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#code for classification model Machine Learning\n",
        "\n",
        "def calculate_accuracy(encoded_df, target):\n",
        "    feature_columns = [col for col in encoded_df.columns if col != target]\n",
        "    encoded_df = encoded_df.dropna(subset=feature_columns)\n",
        "    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "    lr = LogisticRegression(labelCol=target, featuresCol=\"features\")\n",
        "    pipeline = Pipeline(stages=[assembler, lr])\n",
        "    (training_data, testing_data) = encoded_df.randomSplit([0.7, 0.3], seed=42)\n",
        "    model = pipeline.fit(training_data)\n",
        "    predictions = model.transform(testing_data)\n",
        "    evaluator = MulticlassClassificationEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "    accuracy = evaluator.evaluate(predictions)\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "XbpkQeRLm5Ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#asking user to input to input\n",
        "user_input = input(\"Enter 'classification', 'regression' \").lower()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vAvmGZYpiiC",
        "outputId": "b0546901-d3ec-4e7c-a34e-33f45bb7b33a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter 'classification', 'regression' regression\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#target variable will choose which model to go for:\n",
        "if \"classification\" in user_input:\n",
        "    target = input(\"Enter the classification target column name: \")\n",
        "    classification_accuracy = calculate_accuracy(encoded_df, target)\n",
        "    print(\"Classification Accuracy:\", classification_accuracy)\n",
        "\n",
        "if \"regression\" in user_input:\n",
        "    target = input(\"Enter the regression target column name: \")\n",
        "    regression_rmse = calculate_rmse(encoded_df, target)\n",
        "    print(\"Regression RMSE:\", regression_rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkQPnOvbqCTJ",
        "outputId": "7b60a868-1fd5-4217-b8f5-2cbbcbfba474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the regression target column name: medv\n",
            "Regression RMSE: 3.9524689809083613\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AWS GLUE CODE Pre Processing"
      ],
      "metadata": {
        "id": "H-NL_xYJJJGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import boto3\n",
        "from io import BytesIO\n",
        "import sys\n",
        "from awsglue.utils import getResolvedOptions\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from pyspark.sql.functions import col, sum as sum_\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import col, regexp_replace, isnan, when, count, countDistinct, mean, stddev\n",
        "from pyspark.sql.types import StringType, DoubleType, IntegerType\n",
        "import re\n",
        "from pyspark.sql.functions import col, isnan, when, count, mean, stddev, approx_count_distinct\n",
        "\n",
        "\n",
        "def read_target_variable_from_s3(bucket, key):\n",
        "    s3_client = boto3.client('s3')\n",
        "    obj = s3_client.get_object(Bucket=bucket, Key=key)\n",
        "    target_variable = obj['Body'].read().decode('utf-8').strip()\n",
        "    return target_variable\n",
        "\n",
        "\n",
        "def clean_column_name(column_name):\n",
        "    return re.sub(r\"[ ,;{​​​​​​​}​​​​​​​()\\n\\t=]\", \"_\", column_name)\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Initialize Spark session\n",
        "    spark = SparkSession.builder.appName(\"DataFrameAnalysis\").getOrCreate()\n",
        "\n",
        "\n",
        "    # Fetch arguments passed to the Glue job\n",
        "    args = getResolvedOptions(sys.argv, ['INPUT_PATH', 'OUTPUT_BUCKET', 'OUTPUT_PATH'])\n",
        "    input_path = args['INPUT_PATH']\n",
        "    output_bucket = args['OUTPUT_BUCKET']\n",
        "    output_path = args['OUTPUT_PATH']\n",
        "\n",
        "       # Define the target variable file location in S3\n",
        "    # bucket_name = 'mycloudcomputingproject'  # replace with your S3 bucket name\n",
        "    # target_variable_key = 'target_variable.txt'  # consistent file name\n",
        "    # # Read the target variable\n",
        "    # target_variable = read_target_variable_from_s3(bucket_name, target_variable_key)\n",
        "\n",
        "\n",
        "    # Read CSV file from S3\n",
        "    df = spark.read.csv(f\"s3://{​​​​​​​input_path}​​​​​​​\", header=True, inferSchema=True)\n",
        "\n",
        "\n",
        "    # Remove duplicates\n",
        "    df = df.dropDuplicates()\n",
        "\n",
        "\n",
        "    # Clean column names\n",
        "    new_column_names = [clean_column_name(c) for c in df.columns]\n",
        "    df = df.toDF(*new_column_names)\n",
        "\n",
        "\n",
        "    # Descriptive statistics\n",
        "    json_data = {​​​​​​​}​​​​​​​\n",
        "    json_data['total_rows'] = df.count()\n",
        "    column_stats = {​​​​​​​}​​​​​​​\n",
        "\n",
        "\n",
        "    # Identifying categorical and quantitative columns\n",
        "    categorical_columns = []\n",
        "    quantitative_columns = []\n",
        "\n",
        "\n",
        "    for column in df.columns:\n",
        "        column_data = {​​​​​​​}​​​​​​​\n",
        "        column_type = df.schema[column].dataType\n",
        "\n",
        "\n",
        "        if isinstance(column_type, StringType):\n",
        "            categorical_columns.append(column)\n",
        "        elif isinstance(column_type, (DoubleType, IntegerType)):\n",
        "            quantitative_columns.append(column)\n",
        "\n",
        "\n",
        "        column_data['distinct_count'] = df.select(column).distinct().count()\n",
        "        column_data['missing_values'] = df.filter((col(column).isNull()) | (isnan(col(column)))).count()\n",
        "\n",
        "\n",
        "        if isinstance(column_type, (DoubleType, IntegerType)):\n",
        "            column_data['mean'] = df.select(mean(col(column))).collect()[0][0]\n",
        "            column_data['stddev'] = df.select(stddev(col(column))).collect()[0][0]\n",
        "\n",
        "\n",
        "        column_stats[column] = column_data\n",
        "\n",
        "\n",
        "    json_data['column_stats'] = column_stats\n",
        "    json_data['categorical_columns'] = categorical_columns\n",
        "    json_data['quantitative_columns'] = quantitative_columns\n",
        "\n",
        "\n",
        "\n",
        "    # Handling missing values\n",
        "    total_rows = df.count()\n",
        "    if total_rows > 0:\n",
        "\n",
        "\n",
        "        for column in df.columns:\n",
        "            missing_count = df.filter((col(column).isNull()) | (isnan(col(column)))).count()\n",
        "            if missing_count / total_rows <= 0.3:\n",
        "                # If missing values are less than 30%, drop rows\n",
        "                df = df.filter(col(column).isNotNull())\n",
        "            else:\n",
        "                # If more than 30%, impute with median/mode\n",
        "                if column in quantitative_columns:\n",
        "                    median_value = df.approxQuantile(column, [0.5], 0.05)[0]\n",
        "                    df = df.na.fill({​​​​​​​column: median_value}​​​​​​​)\n",
        "                elif column in categorical_columns:\n",
        "                    mode_value = df.groupBy(column).count().orderBy('count', ascending=False).first()[0]\n",
        "                    df = df.na.fill({​​​​​​​column: mode_value}​​​​​​​)\n",
        "\n",
        "\n",
        "        # Update total rows after handling missing values\n",
        "        json_data['rows_after_handling_missing'] = df.count()\n",
        "    else: pass\n",
        "\n",
        "\n",
        "    # Save JSON result to S3\n",
        "    s3_client = boto3.client('s3')\n",
        "    json_key = output_path + input_path.split('/')[-1] + '_summary.json'\n",
        "    s3_client.put_object(Bucket=output_bucket, Key=json_key, Body=json.dumps(json_data, indent=4))\n",
        "\n",
        "\n",
        "    # Stop the Spark session\n",
        "    spark.stop()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "VOTPQaZUGdJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#AWS Lambda function for results"
      ],
      "metadata": {
        "id": "eI76ucGqGdnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "from botocore.exceptions import ClientError\n",
        "import json\n",
        "\n",
        "\n",
        "\n",
        "def lambda_handler(event, context):\n",
        "    if 'id' in event:\n",
        "        unique_id = event['id']\n",
        "    else:\n",
        "        return {\n",
        "            'statusCode': 400,\n",
        "            'body': json.dumps('Missing unique ID parameter.')\n",
        "        }\n",
        "\n",
        "    s3 = boto3.client('s3')\n",
        "    bucket_name = 'mycloudcomputingproject'\n",
        "    results_prefix = f's3://mycloudcomputingproject/results/{unique_id}'  # Adjusted prefix\n",
        "\n",
        "\n",
        "\n",
        "    try:\n",
        "        response = s3.list_objects_v2(Bucket=bucket_name, Prefix=results_prefix)\n",
        "\n",
        "        if 'Contents' in response and response['Contents']:\n",
        "            object_key = response['Contents'][0]['Key']\n",
        "            file = s3.get_object(Bucket=bucket_name, Key=object_key)\n",
        "            file_content = file['Body'].read().decode('utf-8')\n",
        "\n",
        "            # Parse JSON content if the file is a JSON file\n",
        "            if object_key.endswith('.json'):\n",
        "                file_content = json.loads(file_content)\n",
        "\n",
        "\n",
        "\n",
        "            return {\n",
        "                'statusCode': 200,\n",
        "                'body': json.dumps(file_content)\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                'statusCode': 404,\n",
        "                'body': json.dumps('No results found for the provided ID.')\n",
        "            }\n",
        "    except ClientError as e:\n",
        "        return {\n",
        "            'statusCode': 500,\n",
        "            'body': json.dumps(f\"Error fetching results: {str(e)}\")\n",
        "        }"
      ],
      "metadata": {
        "id": "YmhAGdvsI0B0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#AWS lambda Code for uploading"
      ],
      "metadata": {
        "id": "X7mdzyrEI_3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "import json\n",
        "\n",
        "\n",
        "\n",
        "def lambda_handler(event, context):\n",
        "    s3 = boto3.client('s3')\n",
        "    bucket_name = 'mycloudcomputingproject'\n",
        "\n",
        "\n",
        "\n",
        "    file_key = event['queryStringParameters']['filename']\n",
        "    target_variable = event['queryStringParameters']['targetVariable']  # Retrieve the target variable\n",
        "\n",
        "\n",
        "\n",
        "    # Store the target variable in S3\n",
        "    # s3.put_object(Bucket=bucket_name, Key=f\"{file_key}_target_variable\", Body=target_variable)\n",
        "\n",
        "\n",
        "\n",
        "    presigned_url = s3.generate_presigned_url('put_object', Params={'Bucket': bucket_name, 'Key': file_key}, ExpiresIn=3600)\n",
        "\n",
        "\n",
        "    return {\n",
        "        'statusCode': 200,\n",
        "        'headers': {\n",
        "            'Access-Control-Allow-Origin': '*',\n",
        "            'Content-Type': 'application/json'\n",
        "        },\n",
        "        'body': json.dumps({'url': presigned_url})\n",
        "    }"
      ],
      "metadata": {
        "id": "_lkvZdqaI_Uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#HTML Code"
      ],
      "metadata": {
        "id": "xzLX_e0DMUGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>Upload CSV/Excel File</title>\n",
        "    <style>\n",
        "        /* CSS for loading indicator */\n",
        "        #loading-indicator {\n",
        "            display: none;\n",
        "            font-size: 16px;\n",
        "            color: gray;\n",
        "        }\n",
        "    </style>\n",
        "</head>\n",
        "\n",
        "<body>\n",
        "    <h2>Option 1 - Choose a CSV/Excel file</h2>\n",
        "    <label for=\"file-input\" style=\"cursor: pointer;\">\n",
        "        Choose file <span id=\"file-name\">No file chosen</span>\n",
        "    </label>\n",
        "    <input type=\"file\" id=\"file-input\" accept=\".csv, .xls, .xlsx\" style=\"display: none;\">\n",
        "    <button id=\"upload-button\" disabled>Upload</button>\n",
        "\n",
        "    <!-- Dropdown for CSV column headings -->\n",
        "    <div id=\"csv-dropdown-container\" style=\"display: none;\">\n",
        "        <label for=\"csv-column-dropdown\">Select Column:</label>\n",
        "        <select id=\"csv-column-dropdown\"></select>\n",
        "    </div>\n",
        "\n",
        "    <div id=\"results-container\">\n",
        "        <h2>Results</h2>\n",
        "        <div id=\"results\"></div>\n",
        "        <div id=\"loading-indicator\">Loading results...</div> <!-- Loading indicator -->\n",
        "\n",
        "        <!-- Display selected column message -->\n",
        "        <div id=\"selected-column-message\"></div>\n",
        "    </div>\n",
        "\n",
        "    <script>\n",
        "        const fileInput = document.getElementById(\"file-input\");\n",
        "        const fileNameDisplay = document.getElementById(\"file-name\");\n",
        "        const uploadButton = document.getElementById(\"upload-button\");\n",
        "        const csvDropdownContainer = document.getElementById(\"csv-dropdown-container\");\n",
        "        const csvColumnDropdown = document.getElementById(\"csv-column-dropdown\");\n",
        "        const loadingIndicator = document.getElementById(\"loading-indicator\");\n",
        "        const selectedColumnMessage = document.getElementById(\"selected-column-message\");\n",
        "\n",
        "        function generateUniqueId() {\n",
        "            return new Date().getTime().toString(); // Simple timestamp-based ID\n",
        "        }\n",
        "\n",
        "        function showCsvDropdown(file) {\n",
        "            const reader = new FileReader();\n",
        "            reader.onload = function (e) {\n",
        "                const content = e.target.result;\n",
        "                const lines = content.split('\\n');\n",
        "                const columnHeadings = lines[0].split(',');\n",
        "\n",
        "                csvColumnDropdown.innerHTML = \"\";\n",
        "                columnHeadings.forEach((heading, index) => {\n",
        "                    const option = document.createElement(\"option\");\n",
        "                    option.value = index;\n",
        "                    option.text = heading;\n",
        "                    csvColumnDropdown.add(option);\n",
        "                });\n",
        "\n",
        "                csvDropdownContainer.style.display = \"block\";\n",
        "            };\n",
        "            reader.readAsText(file);\n",
        "        }\n",
        "\n",
        "        function uploadFileToS3(file, uniqueId, targetVariable) {\n",
        "            const modifiedFileName = `${uniqueId}_${file.name}`;\n",
        "            fetch(`https://ytu7o2xxgd.execute-api.us-east-2.amazonaws.com/prod/uploading?filename=${encodeURIComponent(modifiedFileName)}&targetVariable=${encodeURIComponent(targetVariable)}`, {\n",
        "                method: 'POST',\n",
        "                headers: {\n",
        "                    'Content-Type': 'application/json'\n",
        "                }\n",
        "            })\n",
        "                .then(response => response.json())\n",
        "                .then(data => {\n",
        "                    let presignedUrl = data.url;\n",
        "                    fetch(presignedUrl, {\n",
        "                        method: 'PUT',\n",
        "                        body: file\n",
        "                    })\n",
        "                        .then(response => {\n",
        "                            if (response.ok) {\n",
        "                                console.log(\"File uploaded successfully.\");\n",
        "                                pollForResults(uniqueId, 0); // Start polling for results\n",
        "                            } else {\n",
        "                                console.error('Error during file upload:', response);\n",
        "                            }\n",
        "                        })\n",
        "                        .catch(uploadError => console.error('Error during file upload:', uploadError));\n",
        "                })\n",
        "                .catch(fetchError => console.error('Error fetching presigned URL:', fetchError));\n",
        "        }\n",
        "\n",
        "        uploadButton.addEventListener(\"click\", () => {\n",
        "            const selectedFile = fileInput.files[0];\n",
        "            if (selectedFile) {\n",
        "                const uniqueId = generateUniqueId();\n",
        "                const selectedColumnValue = csvColumnDropdown.selectedOptions[0].text;\n",
        "                uploadFileToS3(selectedFile, uniqueId, selectedColumnValue);\n",
        "            }\n",
        "        });\n",
        "\n",
        "        function pollForResults(uniqueId, attempts) {\n",
        "            const maxAttempts = 30;\n",
        "            const resultsContainer = document.getElementById(\"results\");\n",
        "            const apiGatewayUrl = `https://8ygx8j5fxd.execute-api.us-east-2.amazonaws.com/prod/result?id=${uniqueId}`;\n",
        "\n",
        "            loadingIndicator.style.display = \"block\";\n",
        "            resultsContainer.textContent = \"\";\n",
        "\n",
        "            function fetchResults() {\n",
        "                fetch(apiGatewayUrl)\n",
        "                    .then(response => {\n",
        "                        if (response.ok) {\n",
        "                            return response.json();\n",
        "                        } else {\n",
        "                            throw new Error(`Response not OK: ${response.statusText}`);\n",
        "                        }\n",
        "                    })\n",
        "                    .then(data => {\n",
        "                        if (data.statusCode !== 404) {\n",
        "                            resultsContainer.textContent = JSON.stringify(data, null, 2);\n",
        "                            loadingIndicator.style.display = \"none\"; // Hide loading indicator\n",
        "                        } else if (attempts < maxAttempts) {\n",
        "                            setTimeout(() => {\n",
        "                                pollForResults(uniqueId, attempts + 1);\n",
        "                            }, 10000); // Wait for 10 seconds before retrying\n",
        "                        } else {\n",
        "                            resultsContainer.textContent = 'No results found after maximum attempts.';\n",
        "                            loadingIndicator.style.display = \"none\"; // Hide loading indicator\n",
        "                        }\n",
        "                    })\n",
        "                    .catch(error => {\n",
        "                        console.error('Error:', error);\n",
        "                        if (attempts < maxAttempts) {\n",
        "                            setTimeout(() => {\n",
        "                                pollForResults(uniqueId, attempts + 1);\n",
        "                            }, 10000);\n",
        "                        } else {\n",
        "                            resultsContainer.textContent = 'Error fetching results after maximum attempts.';\n",
        "                            loadingIndicator.style.display = \"none\";\n",
        "                        }\n",
        "                    });\n",
        "            }\n",
        "\n",
        "            fetchResults();\n",
        "        }\n",
        "\n",
        "        fileInput.addEventListener(\"change\", () => {\n",
        "            const selectedFile = fileInput.files[0];\n",
        "            if (selectedFile) {\n",
        "                fileNameDisplay.textContent = selectedFile.name;\n",
        "                uploadButton.disabled = false;\n",
        "                showCsvDropdown(selectedFile);\n",
        "            } else {\n",
        "                fileNameDisplay.textContent = \"No file chosen\";\n",
        "                uploadButton.disabled = true;\n",
        "                csvDropdownContainer.style.display = \"none\";\n",
        "            }\n",
        "        });\n",
        "    </script>\n",
        "</body>\n",
        "\n",
        "</html>\n"
      ],
      "metadata": {
        "id": "LoJkW4HBMXXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#End of the Code"
      ],
      "metadata": {
        "id": "pDagx63JJUvM"
      }
    }
  ]
}